{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify configuration file\n",
    "\n",
    "All of your Twitter API tokens and keys, and Twitter screen name and password are stored in a file called *scripts/config_{your_name}.py*.  We give you a template file called *scripts/config.py* in the repo.  Change the name of this file to *scripts/config_{your_name}.py*, as we do with homework files.  Then put your Chrome driver path, Twitter username and password, along with your Twiiter API credentials are in the *scripts/config_{your_name}.py* file.  \n",
    "\n",
    "You can find the API credentials for your Twitter API account here: https://developer.twitter.com/en/apps.  Click on *Details* for your app, and then *Keys and Tokens*. \n",
    "\n",
    "The Twitter API credentials are called\n",
    "\n",
    "1. `APP_KEY`\n",
    "\n",
    "2. `APP_SECRET`\n",
    "\n",
    "3. `OAUTH_TOKEN`\n",
    "\n",
    "4. `OAUTH_TOKEN_SECRET`\n",
    "\n",
    "The Twitter login info is called\n",
    "\n",
    "1. `USER`\n",
    "\n",
    "2. `PASSWORD`\n",
    "\n",
    "The Chrome drive path is called\n",
    "\n",
    "1. `DRIVER_PATH`\n",
    "\n",
    "I recommend the `DRIVER_PATH` be something like `DRIVER_PATH = 'scripts/chromedriver_win32/chromedriver.exe'`  Basically, create a folder in the *scripts* folder and put the drive .exe file in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "We will need:\n",
    "1. `twython` - this package lets you connect to the Twitter API. \n",
    "\n",
    "2. `selenium` - this package lets you crawl websites.\n",
    "\n",
    "3. `chromedriver_autoinstaller` - this package installs the chrome driver you download.\n",
    "\n",
    "4. Chrome driver - this is a software that lets us do the webcrawling with `selenium`. You have to download a Chrome driver from https://chromedriver.chromium.org/downloads.  Check to make sure your driver matches your version of Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install twython --upgrade --user\n",
    "!pip install selenium --upgrade --user\n",
    "!pip install chromedriver_autoinstaller --upgrade --user\n",
    "\n",
    "#You need to download your chrome driver too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We will import the packages we installed, along with some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import sqlite3, sys, os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import codecs  #this let's us display tweets properly (emojis, etc.)\n",
    "\n",
    "#helper code\n",
    "import scripts.scraper_twitter_api as api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import configuration file\n",
    "\n",
    "Import your modified configuration file with the code `from scripts.config_{your_name} import *`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.config_lmdisch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect a User's Tweets with the Twitter API\n",
    "This section provides you code to collect the tweets of a user, or a list of users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Twitter API\n",
    "\n",
    "First we create `twitter` which is a `Twython` object that connects us to the Twitter API.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection made to Twitter API for lmdisch\n",
      "type(twitter)= <class 'twython.api.Twython'>\n"
     ]
    }
   ],
   "source": [
    "twitter = Twython(APP_KEY, APP_SECRET,OAUTH_TOKEN, OAUTH_TOKEN_SECRET);\n",
    "print(\"Connection made to Twitter API for \"+twitter.verify_credentials()['screen_name'])\n",
    "print(f\"type(twitter)= {type(twitter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select screen names\n",
    "\n",
    "Create a list `screen_names` that contains all the screen names of Twitter users whose tweets you want to collect.  The tweets will be saved to a database file.  If you have already have a database file that exists, you can add the new data to the existing database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect the tweets for each screen name\n",
    "\n",
    "The `for` loop will go through each `screen_name` in `screen_names`, collect the tweets using the `user_tweets` function, and saves the tweets to a database with filename given by `fname`.\n",
    "\n",
    "You can run this code again later with different users and they will be added to the same database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f\"data/tweets_users.db\"\n",
    "for screen_name in screen_names:    \n",
    "    df1 = api.user_tweets(twitter,screen_name,fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load database into a dataframe\n",
    "\n",
    "We create a connection `conn` to the database and then load the data into a dataframe `df` with the `read_sql_query` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(fname)\n",
    "df = pd.read_sql_query(\"SELECT * FROM tweet\", conn)\n",
    "\n",
    "print(f\"df has {len(df)} rows\")\n",
    "print(df.screen_name.unique())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at mean retweet count per user\n",
    "\n",
    "For fun, why don't we look at the mean retweet counts per user.  We can use the `groupby` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,4))\n",
    "\n",
    "df_plot = df.groupby(by = ['screen_name']).mean()\n",
    "df_plot['screen_name'] = df_plot.index\n",
    "sns.barplot(data = df_plot, x = 'screen_name', y='retweet_count')\n",
    "plt.ylabel(\"Mean retweet count\",fontsize = 16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Tweets by Keyword with the Twitter API\n",
    "\n",
    "Next we will provide code to collect tweets that contain a keyword, or one of many in a set of keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(APP_KEY, APP_SECRET,OAUTH_TOKEN, OAUTH_TOKEN_SECRET);\n",
    "print(\"Connection made to Twitter API for \"+twitter.verify_credentials()['screen_name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list of query keywords\n",
    "\n",
    "Create a list `keywords` that has all the words you want to search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['boycottchina', 'CCPvirus', 'chinavirus', 'chinesevirus', 'chinaliedpeopledied', \n",
    "            'ChinaEnemyToTheWorld', 'kungflu', 'wuhanvirus', 'xivirus', 'beijingbiden', 'bidenvirus',\n",
    "            'CovidHOAX', 'PLANdemic', 'Scamdemic', 'MedicalMartialLaw', 'stopaapihate', 'stopasianhate', \n",
    "            'stopasianhatecrimes', 'racism', 'asianstrong', 'asianpride', 'asianamerican', 'webelonghere', \n",
    "            'protectourelders', 'aapi', 'standforasians', 'wearenotavirus', 'indianvariant', 'southafricanvariant', \n",
    "            'britishvariant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect tweets for each keyword\n",
    "\n",
    "The tweets will be saved in a database file with name given by `fname`.  If you run this cell again with the same filename, new tweets will be added to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets will be saved to database data/final_project_tweets.db\n",
      "Querying keyword  boycottchina\n",
      "Insterting final batch of tweets. got  1024  to insert\n",
      "Querying keyword  CCPvirus\n",
      "Insterting final batch of tweets. got  1015  to insert\n",
      "Querying keyword  chinavirus\n",
      "Insterting final batch of tweets. got  1064  to insert\n",
      "Querying keyword  chinesevirus\n",
      "Insterting final batch of tweets. got  1080  to insert\n",
      "Querying keyword  chinaliedpeopledied\n",
      "Insterting final batch of tweets. got  1074  to insert\n",
      "Querying keyword  ChinaEnemyToTheWorld\n",
      "No more results\n",
      "Insterting final batch of tweets. got  433  to insert\n",
      "Querying keyword  kungflu\n",
      "No more results\n",
      "Insterting final batch of tweets. got  137  to insert\n",
      "Querying keyword  wuhanvirus\n",
      "Insterting final batch of tweets. got  1006  to insert\n",
      "Querying keyword  xivirus\n",
      "No more results\n",
      "Insterting final batch of tweets. got  8  to insert\n",
      "Querying keyword  beijingbiden\n",
      "No more results\n",
      "Insterting final batch of tweets. got  602  to insert\n",
      "Querying keyword  bidenvirus\n",
      "No more results\n",
      "Insterting final batch of tweets. got  13  to insert\n",
      "Querying keyword  CovidHOAX\n",
      "No more results\n",
      "Insterting final batch of tweets. got  558  to insert\n",
      "Querying keyword  PLANdemic\n",
      "Insterting final batch of tweets. got  1078  to insert\n",
      "Querying keyword  Scamdemic\n",
      "Insterting final batch of tweets. got  1043  to insert\n",
      "Querying keyword  MedicalMartialLaw\n",
      "No more results\n",
      "Insterting final batch of tweets. got  35  to insert\n",
      "Querying keyword  stopaapihate\n",
      "Insterting final batch of tweets. got  1000  to insert\n",
      "Querying keyword  stopasianhate\n",
      "Insterting final batch of tweets. got  1000  to insert\n",
      "Querying keyword  stopasianhatecrimes\n",
      "Insterting final batch of tweets. got  1087  to insert\n",
      "Querying keyword  racism\n",
      "Insterting final batch of tweets. got  1092  to insert\n",
      "Querying keyword  asianstrong\n",
      "No more results\n",
      "Insterting final batch of tweets. got  4  to insert\n",
      "Querying keyword  asianpride\n",
      "No more results\n",
      "Insterting final batch of tweets. got  390  to insert\n",
      "Querying keyword  asianamerican\n",
      "Insterting final batch of tweets. got  1000  to insert\n",
      "Querying keyword  webelonghere\n",
      "No more results\n",
      "Insterting final batch of tweets. got  316  to insert\n",
      "Querying keyword  protectourelders\n",
      "No more results\n",
      "Insterting final batch of tweets. got  23  to insert\n",
      "Querying keyword  aapi\n",
      "Insterting final batch of tweets. got  1000  to insert\n",
      "Querying keyword  standforasians\n",
      "\tToo many requests, go sleep for 15 minutes\n",
      "\tWill start insterting tweets in the meantime, got  100  to insert\n",
      "No more results\n",
      "Insterting final batch of tweets. got  232  to insert\n",
      "Querying keyword  wearenotavirus\n",
      "No more results\n",
      "Insterting final batch of tweets. got  5  to insert\n",
      "Querying keyword  indianvariant\n",
      "No more results\n",
      "Insterting final batch of tweets. got  140  to insert\n",
      "Querying keyword  southafricanvariant\n",
      "No more results\n",
      "Insterting final batch of tweets. got  138  to insert\n",
      "Querying keyword  britishvariant\n",
      "No more results\n",
      "Insterting final batch of tweets. got  2  to insert\n",
      "16628 tweets in database\n"
     ]
    }
   ],
   "source": [
    "fname = f\"data/final_project_tweets.db\"\n",
    "max_tweets = 1000\n",
    "df = api.keyword_tweets(twitter ,keywords,fname,max_tweets = max_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load database into a dataframe\n",
    "\n",
    "We create a connection `conn` to the database and then load the data into a dataframe with the `read_sql_query` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has 16628 rows\n",
      "df has columns Index(['tweet_id', 'user_id', 'screen_name', 'created_at', 'text', 'geo_lat',\n",
      "       'geo_long', 'place_type', 'place_name', 'lang', 'source',\n",
      "       'retweet_count', 'favorite_count', 'retweet_status_id',\n",
      "       'reply_to_status_id', 'reply_to_user_id', 'reply_to_screen_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(fname)\n",
    "df = pd.read_sql_query(\"SELECT * FROM tweet\", conn)\n",
    "\n",
    "print(f\"df has {len(df)} rows\")\n",
    "print(f\"df has columns {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at top retweeted tweets\n",
    "\n",
    "For fun, let's print out the top retweeted tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010571 retweets: @ktrina30776898: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @95KTH_____: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @TanyaBTSArmy3: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @missrightmaya: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @kth4levi: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @lixttps: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @AdrianoAlexa: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @alz_dani: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @hollyoongi93_: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n",
      "1010571 retweets: @autumsworId: RT @BTS_twt: #StopAsianHate\n",
      "#StopAAPIHate https://t.co/mOmttkOpOt \n"
     ]
    }
   ],
   "source": [
    "ndisplay = 10\n",
    "c = 0\n",
    "for index, row in df.sort_values(by = ['retweet_count'],ascending = False).iterrows():\n",
    "    c+=1\n",
    "    text = codecs.decode(row.text, 'unicode_escape')\n",
    "    print(f\"{row.retweet_count} retweets: @{row.screen_name}: {text}\")\n",
    "    if c>=ndisplay:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Following Networks with Web Crawlers\n",
    "\n",
    "The `followers` and `following` modules contain functions to collect the followers and following of users using a web crawler.  We don't use the Twitter API because it is incredibly slow for collecting network data.\n",
    "\n",
    "When building your networks, it is easier to use the `following` module.  This way you avoid getting stuck on someone with 100 million followers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify followers module.\n",
    "\n",
    "The `following` and `followers` modules need to import your Twitter user name and password from the configuration file.  Since you will change the name of this file to *config_{your_name}.py*, you need to change the import line in *following.py* and *followers.py* from `from scripts.config import *` to  `from scripts.config_{your_name} import *`.  Then run the code `import scripts.following as Following` and `import scripts.followers as Followers`.\n",
    "\n",
    "*ANNOYING FACT*: Each time you hard reset your repo, the *followers.py* and *following.py* files are overwritten with the version on the repo.  This means you have to change this import line each time you do a hard reset.  If you are clever, maybe you can rename the files and find a way to import them. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.following as Following\n",
    "import scripts.followers as Followers\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect the following of a list of users.\n",
    "\n",
    "Create a list `screen_name` of all the screen names you want to collect following for.  The function `Following.Network.multi_fetch` will collect the following for each screen name.  This data is returned as a dataframe `df`, whose columns are `screen_name` and `following`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "screen_names = [\"JoeBiden\",\"JanetYellen\",\"POTUS\",\"SecDef\",\"KamalaHarris\",\"DrBiden\", \"BarackObama\"]\n",
    "df = Following.Network.multi_fetch(users=screen_names,max_count = 500)\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    print(f\"{row.screen_name}: {len(row.following)} following\")\n",
    "\n",
    "df.head()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create networkx object\n",
    "\n",
    "This code creates a networkx object `G` from `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for index,row in df.iterrows():\n",
    "    u = row.screen_name\n",
    "    G.add_node(u)\n",
    "    for v in row.following:\n",
    "        if v in df.screen_name.tolist():\n",
    "            G.add_edge(v,u)\n",
    "            \n",
    "print(f\"Network has {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your network\n",
    "\n",
    "Save the networkx object `G` to a pickle file with name given by `fname` using the `write_gpickle` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/network_following_biden.pickle'\n",
    "nx.write_gpickle(G,fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load network and draw it\n",
    "\n",
    "Just to make sure we did everything correctly, load the network using the `read_gpickle` function, and draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_network_pos(G,pos,title_str):\n",
    "    node_size = 100\n",
    "    node_color = \"pink\"\n",
    "    width = 0.5\n",
    "    edge_color = \"white\"\n",
    "    bg_color = \"black\"\n",
    "\n",
    "    #2 points  drawing network with directed layout \n",
    "    fig = plt.figure(figsize= (8,6))\n",
    "    plt.subplot(1,1,1)\n",
    "    nx.draw(G, width=width,pos=pos ,node_color=node_color,\n",
    "            edge_color=edge_color,node_size=node_size,\n",
    "            connectionstyle='arc3',with_labels=True,font_color = 'white')\n",
    "    plt.title(title_str,color = \"white\")\n",
    "    fig.set_facecolor(bg_color)\n",
    "    plt.show()    \n",
    "    return 1\n",
    "\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "draw_network_pos(G,pos,\"Biden Following Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
